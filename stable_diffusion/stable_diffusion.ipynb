{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c10ed41-57e2-4e7f-a34d-16c94dccbb67",
   "metadata": {},
   "source": [
    "# GUI for Stable Diffusion\n",
    "\n",
    "# Stable Diffusion\n",
    "Written by Jasmine Sandhu (Acknowledgements: Jim Bednar, Maxime Liquet, Philipp Rudiger)<br>\n",
    "Created: Jan, 2023<br>\n",
    "Last updated: Jan, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35069f42-8e53-4644-9aec-6e15f883e846",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Stable Diffusion, Diffusers library\n",
    "\n",
    "[Stable Diffusion](https://en.wikipedia.org/wiki/Stable_Diffusion#:~:text=Stable%20Diffusion%20is%20a%20deep,guided%20by%20a%20text%20prompt) is a deep learning, text-to-image model released in 2022. It is primarily used to generate detailed images conditioned on text descriptions. \n",
    "\n",
    "This example uses the [Diffusers library](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb) with checkpoints from the runwayml and CompVis repositories. [Diffusers on github](https://github.com/huggingface/diffusers#stable-diffusion-is-fully-compatible-with-diffusers). Blogpost on [Stable Diffusion with Diffusers](https://huggingface.co/blog/stable_diffusion)\n",
    "\n",
    "### Performance: GPU\n",
    "\n",
    "The example assumes it will run on a GPU. It can be modified to run on a CPU but image generation will take on the order of minutes as opposed to seconds.\n",
    "\n",
    "\n",
    "### Limitations\n",
    "\n",
    "The models were trained on images with resolution of 512x512. The diffusers pipeline and subsequently the UI allows creation of images with different resolutions; however, the image quality degrades if deviating from the resolution used to train the model. \n",
    "\n",
    "\n",
    "### Seed\n",
    "\n",
    "The idea behind stable diffusion is to start with a noisy image, with the goal of removing gaussian noise in each inference step. The seed value determines the randomness and the output generated. By default the seed is randomized in this application with the opportunity to explore generated images for the same prompt. Fixing the seed will recreate the same image for a given resolution. As noted above, changing the resolution will also change the image output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1227ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import torch\n",
    "import random\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "from bokeh.models.formatters import PrintfTickFormatter\n",
    "import panel as pn\n",
    "import param\n",
    "from panel.layout.base import ListLike\n",
    "from panel.reactive import ReactiveHTML\n",
    "from panel.viewable import Viewer, Viewable\n",
    "\n",
    "pn.extension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211fb06b-cdf8-491d-abf8-55569da913fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a context manager to measure execution time and print it to the console\n",
    "@contextmanager\n",
    "def exec_time(description=\"Task\"):\n",
    "    st = time.perf_counter()\n",
    "    yield \n",
    "    print(f\"{description}: {time.perf_counter() - st:.2f} sec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3659bb1-aa2c-4f30-b2c9-f4957bdb2b81",
   "metadata": {},
   "source": [
    "The `init_model` function will first look in the default cache location used by huggingface to find downloaded pretrained model. If these haven't been downloaded yet, it will first download the models. On subsequent restarts of the app, it'll load the models from the local cache. These can also be downloaded separately as follows:\n",
    "  \n",
    "  ```\n",
    "  pipe, cache = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", return_cached_folder=True, local_files_only=False)\n",
    "  pipe, cache = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", return_cached_folder=True, local_files_only=False)\n",
    "  print(cache) # to see the default cache location\n",
    "  ```\n",
    "\n",
    "In addition to caching the pretrained model, we also initialize and cache the diffusers pipeline inside `panel.state.cache`. This ensures that each new visitor to the page does not require creating and destroying a new diffusers pipeline.\n",
    "The initial page load takes an extra ~10 sec or so and allocates the GPU memory required to load the pipeline in memory but subsequent visitors get this pipeline from panel's cache. The memory overhead from here is the amount needed to generate the image  text prompt.\n",
    "Below is an example output of the `nvidia-smi` running on a machine with 2 Quadro RTX 8000 GPUs, after both models load.\n",
    "\n",
    "```\n",
    "+-----------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
    "|-------------------------------+----------------------+----------------------+\n",
    "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                               |                      |               MIG M. |\n",
    "|===============================+======================+======================|\n",
    "|   0  Quadro RTX 8000     Off  | 00000000:15:00.0 Off |                  Off |\n",
    "| 33%   33C    P8    24W / 260W |     48MiB / 49152MiB |      0%      Default |\n",
    "|                               |                      |                  N/A |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "|   1  Quadro RTX 8000     Off  | 00000000:2D:00.0 Off |                  Off |\n",
    "| 33%   40C    P8    29W / 260W |   5933MiB / 49152MiB |      0%      Default |\n",
    "|                               |                      |                  N/A |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "\n",
    "+-----------------------------------------------------------------------------+\n",
    "| Processes:                                                                  |\n",
    "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
    "|        ID   ID                                                   Usage      |\n",
    "|=============================================================================|\n",
    "|    0   N/A  N/A      2024      G   /usr/lib/xorg/Xorg                 23MiB |\n",
    "|    0   N/A  N/A      2545      G   /usr/bin/gnome-shell               20MiB |\n",
    "|    1   N/A  N/A      2024      G   /usr/lib/xorg/Xorg                  4MiB |\n",
    "|    1   N/A  N/A   2263594      C   .../diffusers/bin/python3.11     5925MiB |\n",
    "+-----------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75f0c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize models and define function for image generation\n",
    "# use only downloaded models\n",
    "random_int_range = 1, int(1e6)\n",
    "def init_model(model, gpu_id=1, torch_dtype=None, local_files_only=True):\n",
    "    print(f\"Init model: {model}\")\n",
    "    if torch_dtype:\n",
    "        pipe = StableDiffusionPipeline.from_pretrained(model, torch_dtype=torch_dtype,\n",
    "                                                       local_files_only=local_files_only)\n",
    "    else:\n",
    "        pipe = StableDiffusionPipeline.from_pretrained(model, local_files_only=local_files_only)\n",
    "        \n",
    "\n",
    "    # can use nvidia-smi to check and set this so you're not running on the same one as panel serve\n",
    "    # it just makes it a little more responsive\n",
    "    if torch.cuda.is_available():\n",
    "        pipe.to(f\"cuda:{gpu_id}\")\n",
    "    return pipe     \n",
    "\n",
    "\n",
    "if 'pipelines' in pn.state.cache:\n",
    "    print(f\"load from cache\")\n",
    "    pipelines = pn.state.cache['pipelines']\n",
    "    pseudo_rand_gen = pn.state.cache['pseudo_rand_gen']\n",
    "else:\n",
    "    models = ['runwayml/stable-diffusion-v1-5', \n",
    "              'CompVis/stable-diffusion-v1-4'\n",
    "             ]\n",
    "    \n",
    "    with exec_time(\"Load models\"):\n",
    "        pipelines = dict()\n",
    "        for m in models:\n",
    "            try: \n",
    "                pipelines[m] = init_model(m, torch_dtype=torch.float16)\n",
    "            except OSError:\n",
    "                pipelines[m] = init_model(m, torch_dtype=torch.float16, local_files_only=False)\n",
    "            \n",
    "    if torch.cuda.is_available():\n",
    "        pseudo_rand_gen = torch.Generator(device='cuda')\n",
    "    else:\n",
    "        pseudo_rand_gen = torch.Generator()\n",
    "\n",
    "    pn.state.cache['pipelines'] = pipelines\n",
    "    pn.state.cache['pseudo_rand_gen'] = pseudo_rand_gen\n",
    "    print(f\"Save to cache\")\n",
    "\n",
    "default_model = next(iter(pipelines))\n",
    "    \n",
    "def generate_image(\n",
    "    prompt,\n",
    "    negative_prompt=None,\n",
    "    model=default_model,\n",
    "    height=512,\n",
    "    width=512,\n",
    "    guidance_scale=7.5,\n",
    "    num_steps=30,\n",
    "    seed=None,\n",
    "):\n",
    "    pipe = pipelines[model]\n",
    "    \n",
    "    if not seed or seed < random_int_range[0]:\n",
    "        seed = random.randint(*random_int_range)\n",
    "    \n",
    "    generator = pseudo_rand_gen.manual_seed(seed)\n",
    "    res = pipe(prompt=prompt,\n",
    "               negative_prompt=negative_prompt,\n",
    "               guidance_scale=guidance_scale,\n",
    "               height=height,\n",
    "               width=width,\n",
    "               num_inference_steps=num_steps,\n",
    "               generator=generator,\n",
    "              )\n",
    "    return res.images[0], seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940ce029-7e18-47cb-a3c9-b4dbb3a40690",
   "metadata": {},
   "source": [
    "The various panel widgets in this code block affect the image generation. When rendered with a template, the sidebar should ideally start out collapsed with only the `Prompt` text box visible. A user writes a prompt, hits enter which triggers the callback to invoke the image generation function. Opening the sidebar provides more options. A user can set various options, then click `Generate` to create image with those options or hit enter on the prompt. If the prompt does not change, hitting enter will not generate a new image - use the `Generate` button to create new images with the same prompt. Below is a description of each option.\n",
    "\n",
    "\n",
    "__Prompt__: Enter a text you wish to use for image generation. Some examples below:\n",
    "\n",
    "  1. Wildflowers on a mountain side \n",
    "  1. A dream of a distant planet, with multiple moons\n",
    "  1. valley of flowers in the Himalayas\n",
    "  \n",
    "__Negative Prompt__: Negative prompt is what the model will try to remove from the image. For instance, in example (1) above, you can add `yellow` to negative prompt to remove yellow flowers\n",
    "\n",
    "__Pretrained Model__: These are the models, download from hugging face, used for inference.\n",
    "\n",
    "__Height, Width__: Height and width in pixels of the images.\n",
    "\n",
    "__Guidance Scale__: Also known as CFG (Classifier-free guidance scale). Typically use a value between 7 to 8.5. As you increase this value, the model will try to match the prompt at the expense of image quality or diversity of the image.\n",
    "\n",
    "__# of steps__: The number of denoising steps taken by the model. As you increase the number of steps the image gets more refined; however, it takes longer to generate.\n",
    "\n",
    "The random seed used when create the noise for the image is randomly set for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3968a727",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gallery(ListLike, ReactiveHTML):\n",
    "    \n",
    "    objects = param.List(item_type=Viewable)\n",
    "   \n",
    "    current = param.Integer(default=None)\n",
    "    \n",
    "    margin = param.Integer(0)\n",
    "\n",
    "    _template = \"\"\"\n",
    "    <div id=\"gallery\" style=\"display: flex; flex-direction: row;\">\n",
    "    {% for img in objects %}\n",
    "      <div id=\"img\" name=\"{{ img.name }}\" onclick=${script('click')}>${img}</div>\n",
    "    {% endfor %}\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    _scripts = {\n",
    "        'click': \"\"\"\n",
    "          const id = event.target.parentNode.parentNode.parentNode.id;\n",
    "          data.current = Number(id.split('-')[1]);\n",
    "          \"\"\"\n",
    "    }\n",
    "\n",
    "class StableDiffusionUI(Viewer):\n",
    "    \n",
    "    prompt = param.String(label='Prompt')\n",
    "    \n",
    "    neg_prompt = param.String(label='Negative Prompt')\n",
    "    \n",
    "    model = param.Selector(objects=list(pipelines), default=default_model)\n",
    "    \n",
    "    _size_range = tuple(448 + i*2**6 for i in range(10))\n",
    "    width = param.Selector(_size_range, default=_size_range[1])\n",
    "    \n",
    "    height = param.Selector(_size_range, default=_size_range[1])\n",
    "    \n",
    "    guidance_scale = param.Number(bounds=(5, 10), step=0.1, default=7.5)\n",
    "    \n",
    "    num_steps = param.Integer(label='# of steps', bounds=(10, 75), default=30)\n",
    "    \n",
    "    gallery = param.ClassSelector(class_=Gallery, default=Gallery(min_height=100), precedence=-1)\n",
    "    \n",
    "    seed = param.Integer(\n",
    "        default=random.randint(*random_int_range), bounds=random_int_range, step=10,\n",
    "        precedence=-1)\n",
    "    \n",
    "    generate = param.Event(precedence=1)\n",
    "    \n",
    "    def __init__(self, **params):\n",
    "        self.history = []\n",
    "        super().__init__(name=params.pop('name', 'Stable Diffusion with Panel UI'), **params)\n",
    "        self.gallery.param.watch(self._restore_history, 'current')\n",
    "        self._restore = False\n",
    "        self._on_load()\n",
    "\n",
    "    @contextmanager\n",
    "    def _toggle(self, attr, value):\n",
    "        # toggle state of bool attribute inside context\n",
    "        # if exception raised by code inside the contextmanager, set the state back to original and rethrow exception\n",
    "        init_state = getattr(self, attr)\n",
    "        try:\n",
    "            setattr(self, attr, not(init_state))\n",
    "            yield\n",
    "            setattr(self, attr, not(getattr(self, attr)))\n",
    "        except Exception as ex:\n",
    "            setattr(self, attr, init_state)\n",
    "            raise ex\n",
    "        \n",
    "\n",
    "    def _restore_history(self, event):\n",
    "        if event.new is None:\n",
    "            return\n",
    "        self.gallery.current = None\n",
    "        self.param.update(self.history[event.new])\n",
    "        print(f\"before with - self._restore: {self._restore}\")\n",
    "        with self._toggle('_restore', value=True):\n",
    "            print(f\"before trigger - self._restore: {self._restore}\")\n",
    "            self.param.trigger('generate')\n",
    "        print(f\"after with - self._restore: {self._restore}\")\n",
    " \n",
    "    @property\n",
    "    def _current_state(self):\n",
    "        return {'prompt': self.prompt,\n",
    "                'negative_prompt': self.neg_prompt,\n",
    "                'model': self.model,\n",
    "                'height': self.height,\n",
    "                'width': self.width,\n",
    "                'guidance_scale': self.guidance_scale,\n",
    "                'num_steps': self.num_steps,\n",
    "                'seed': self.seed}\n",
    "    \n",
    "#     def _on_load(self):\n",
    "#         self._generate(restore=pn.state.location.query_params)\n",
    "\n",
    "    \n",
    "#     @param.depends('generate', watch=True)\n",
    "#     def _generate(self, restore=None):\n",
    "#         if restore:\n",
    "#             self.param.update(restore)\n",
    "#         else:\n",
    "#             self.seed = random.randint(*self.param.seed.bounds)\n",
    "\n",
    "#         state = self._current_state\n",
    "#         image, _ = generate_image(**state)\n",
    "#         pn.state.location.update_query(**state)\n",
    "    \n",
    "    def _on_load(self):\n",
    "        if pn.state.location.query_params:\n",
    "            # reuse _restore flag for now to generate new seed or not\n",
    "            self.param.update(pn.state.location.query_params)\n",
    "            print(f\"pn.state.location.query_params: {pn.state.location.query_params}\")\n",
    "            print(f\"before with - self._restore: {self._restore}\")\n",
    "            \n",
    "            with self._toggle('_restore', value=True):\n",
    "                print(f\"before trigger - self._restore: {self._restore}\")\n",
    "                self.param.trigger('generate')\n",
    "            print(f\"after with - self._restore: {self._restore}\")\n",
    "\n",
    "    @param.depends('generate')\n",
    "    def image(self):\n",
    "        if not self.prompt:\n",
    "            return pn.pane.PNG(style={'border': '1px solid black'}, height=self.height, width=self.width)\n",
    "        \n",
    "        if not self._restore:\n",
    "            print(f\"In image - self.seed {self.seed}\")\n",
    "            self.seed = random.randint(*self.param.seed.bounds)\n",
    "\n",
    "        current_state = self._current_state\n",
    "        print(f\"{current_state}\")\n",
    "        image, image_seed = generate_image(**current_state)\n",
    "\n",
    "        if not self._restore:\n",
    "            self.gallery.append(pn.pane.PNG(image.resize((100, 100))))\n",
    "            self.history.append({\n",
    "                p: v for p, v in self.param.values().items() if p not in ('name', 'gallery', 'generate')\n",
    "            })\n",
    "\n",
    "        pn.state.location.update_query(**self._current_state)\n",
    "        return pn.pane.PNG(image, style={'border': '1px solid black'})\n",
    "\n",
    "    def _sidebar_widgets(self):\n",
    "        return [\n",
    "\n",
    "            self.param.model,\n",
    "            pn.Param(self.param.height, widgets={'height': pn.widgets.DiscreteSlider}),\n",
    "            pn.Param(self.param.width, widgets={'width': pn.widgets.DiscreteSlider}),\n",
    "            pn.Param(self.param.guidance_scale, \n",
    "                     widgets={'guidance_scale': \n",
    "                              {'formatter': PrintfTickFormatter(format='%.1f')}}),\n",
    "            self.param.num_steps,\n",
    "        ]\n",
    "\n",
    "    def _main_widgets(self):\n",
    "        return [\n",
    "            pn.Row(\n",
    "                pn.Column(self.param.prompt, self.param.neg_prompt, sizing_mode='stretch_width'),\n",
    "                pn.Param(self.param.generate, \n",
    "                         widgets={'generate': {'button_type': 'success', 'height': 110, 'width': 30, 'name': 'Generate Image'}}),\n",
    "            ),\n",
    "            pn.Row(pn.param.ParamMethod(self.image, loading_indicator=True),\n",
    "                   pn.Column(self.gallery))\n",
    "        ]\n",
    "    \n",
    "    def __panel__(self):\n",
    "        # Discrete slider for width, height: https://huggingface.co/blog/stable_diffusion\n",
    "        return pn.Row(\n",
    "            pn.Column(*self._sidebar_widgets()),\n",
    "            pn.Column(*self._main_widgets(), sizing_mode='stretch_width')\n",
    "        )\n",
    "\n",
    "    \n",
    "sdui = StableDiffusionUI()\n",
    "\n",
    "sdui"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a5b292",
   "metadata": {},
   "source": [
    "### Use a template\n",
    "\n",
    "Use a template to get a clean look and feel.\n",
    "\n",
    "TODO: Start out with the sidebar collapsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6656725f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## logo / headers / \n",
    "logo  = \"\"\"<a href=\"http://panel.pyviz.org\">\n",
    "           <img src=\"https://panel.pyviz.org/_static/logo_stacked.png\" \n",
    "            width=150 height=127 align=\"left\" margin=20px>\"\"\"\n",
    "\n",
    "desc = pn.pane.HTML(\"\"\"\n",
    "    The <a href=\"http://panel.pyviz.org\">Panel</a> library from <a href=\"https://holoviz.org/\">HoloViz</a> \n",
    "    lets you make widget-controlled apps. Here you can use the\n",
    "    <a href=\"https://huggingface.co/docs/diffusers/index\">diffusers</a> library to\n",
    "    generate images from pretrained diffusion models. Panel is used to create the UI for the pipeline.\"\"\", width=250)\n",
    "\n",
    "template = pn.template.MaterialTemplate(\n",
    "    title=sdui.name,\n",
    ")\n",
    "\n",
    "template.sidebar.append(logo)\n",
    "template.sidebar.append(desc.clone(width=300, margin=(20, 5)))\n",
    "template.sidebar.append(pn.Column(*sdui._sidebar_widgets()))\n",
    "\n",
    "template.main.append(pn.Column(*sdui._main_widgets(), sizing_mode='stretch_width'))\n",
    "\n",
    "template.servable();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69cd27b-49f6-41d7-9321-2cb12b017a07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

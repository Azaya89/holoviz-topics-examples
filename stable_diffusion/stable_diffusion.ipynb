{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c10ed41-57e2-4e7f-a34d-16c94dccbb67",
   "metadata": {},
   "source": [
    "# GUI for Stable Diffusion\n",
    "\n",
    "# Stable Diffusion\n",
    "Written by Jasmine Sandhu (Acknowledgements: Jim Bednar, Maxime Liquet, Philipp Rudiger)<br>\n",
    "Created: Jan, 2023<br>\n",
    "Last updated: Jan, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35069f42-8e53-4644-9aec-6e15f883e846",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Stable Diffusion, Diffusers library\n",
    "\n",
    "[Stable Diffusion](https://en.wikipedia.org/wiki/Stable_Diffusion#:~:text=Stable%20Diffusion%20is%20a%20deep,guided%20by%20a%20text%20prompt) is a deep learning, text-to-image model released in 2022. It is primarily used to generate detailed images conditioned on text descriptions. \n",
    "\n",
    "This example uses the [Diffusers library](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb) with checkpoints from the runwayml and CompVis repositories. [Diffusers on github](https://github.com/huggingface/diffusers#stable-diffusion-is-fully-compatible-with-diffusers). Blogpost on [Stable Diffusion with Diffusers](https://huggingface.co/blog/stable_diffusion)\n",
    "\n",
    "### Performance: GPU\n",
    "\n",
    "The example assumes it will run on a GPU. It can be modified to run on a CPU but image generation will take on the order of minutes as opposed to seconds.\n",
    "\n",
    "\n",
    "### Limitations\n",
    "\n",
    "The models were trained on images with resolution of 512x512. The diffusers pipeline and subsequently the UI allows creation of images with different resolutions; however, the image quality degrades if deviating from the resolution used to train the model. \n",
    "\n",
    "\n",
    "### Seed\n",
    "\n",
    "The idea behind stable diffusion is to start with a noisy image, with the goal of removing gaussian noise in each inference step. The seed value determines the randomness and the output generated. By default the seed is randomized in this application with the opportunity to explore generated images for the same prompt. Fixing the seed will recreate the same image for a given resolution. As noted above, changing the resolution will also change the image output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1227ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import deque\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import torch\n",
    "import random\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "import panel as pn\n",
    "from bokeh.models.formatters import PrintfTickFormatter\n",
    "\n",
    "pn.extension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211fb06b-cdf8-491d-abf8-55569da913fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a context manager to measure execution time and print it to the console\n",
    "@contextmanager\n",
    "def exec_time(description=\"Task\"):\n",
    "    st = time.perf_counter()\n",
    "    yield \n",
    "    print(f\"{description}: {time.perf_counter() - st:.2f} sec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3659bb1-aa2c-4f30-b2c9-f4957bdb2b81",
   "metadata": {},
   "source": [
    "The `init_model` function will first look in the default cache location used by huggingface to find downloaded pretrained model. If these haven't been downloaded yet, it will first download the models. On subsequent restarts of the app, it'll load the models from the local cache. These can also be downloaded separately as follows:\n",
    "  \n",
    "  ```\n",
    "  pipe, cache = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", return_cached_folder=True, local_files_only=False)\n",
    "  pipe, cache = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", return_cached_folder=True, local_files_only=False)\n",
    "  print(cache) # to see the default cache location\n",
    "  ```\n",
    "\n",
    "In addition to caching the pretrained model, we also initialize and cache the diffusers pipeline inside `panel.state.cache`. This ensures that each new visitor to the page does not require creating and destroying a new diffusers pipeline.\n",
    "The initial page load takes an extra ~10 sec or so and allocates the GPU memory required to load the pipeline in memory but subsequent visitors get this pipeline from panel's cache. The memory overhead from here is the amount needed to generate the image  text prompt.\n",
    "Below is an example output of the `nvidia-smi` running on a machine with 2 Quadro RTX 8000 GPUs, after both models load.\n",
    "\n",
    "```\n",
    "+-----------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
    "|-------------------------------+----------------------+----------------------+\n",
    "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                               |                      |               MIG M. |\n",
    "|===============================+======================+======================|\n",
    "|   0  Quadro RTX 8000     Off  | 00000000:15:00.0 Off |                  Off |\n",
    "| 33%   33C    P8    24W / 260W |     48MiB / 49152MiB |      0%      Default |\n",
    "|                               |                      |                  N/A |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "|   1  Quadro RTX 8000     Off  | 00000000:2D:00.0 Off |                  Off |\n",
    "| 33%   40C    P8    29W / 260W |   5933MiB / 49152MiB |      0%      Default |\n",
    "|                               |                      |                  N/A |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "\n",
    "+-----------------------------------------------------------------------------+\n",
    "| Processes:                                                                  |\n",
    "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
    "|        ID   ID                                                   Usage      |\n",
    "|=============================================================================|\n",
    "|    0   N/A  N/A      2024      G   /usr/lib/xorg/Xorg                 23MiB |\n",
    "|    0   N/A  N/A      2545      G   /usr/bin/gnome-shell               20MiB |\n",
    "|    1   N/A  N/A      2024      G   /usr/lib/xorg/Xorg                  4MiB |\n",
    "|    1   N/A  N/A   2263594      C   .../diffusers/bin/python3.11     5925MiB |\n",
    "+-----------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75f0c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize models and define function for image generation\n",
    "# use only downloaded models\n",
    "random_int_range = 1, int(1e6)\n",
    "def init_model(model, gpu_id=1, torch_dtype=None, local_files_only=True):\n",
    "    print(f\"Init model: {model}\")\n",
    "    if torch_dtype:\n",
    "        pipe = StableDiffusionPipeline.from_pretrained(model, torch_dtype=torch_dtype,\n",
    "                                                       local_files_only=local_files_only)\n",
    "    else:\n",
    "        pipe = StableDiffusionPipeline.from_pretrained(model, local_files_only=local_files_only)\n",
    "        \n",
    "\n",
    "    # can use nvidia-smi to check and set this so you're not running on the same one as panel serve\n",
    "    # it just makes it a little more responsive\n",
    "    if torch.cuda.is_available():\n",
    "        pipe.to(f\"cuda:{gpu_id}\")\n",
    "    return pipe     \n",
    "\n",
    "\n",
    "if 'pipelines' in pn.state.cache:\n",
    "    print(f\"load from cache\")\n",
    "    pipelines = pn.state.cache['pipelines']\n",
    "    pseudo_rand_gen = pn.state.cache['pseudo_rand_gen']\n",
    "else:\n",
    "    models = ['runwayml/stable-diffusion-v1-5', \n",
    "              'CompVis/stable-diffusion-v1-4'\n",
    "             ]\n",
    "    \n",
    "    with exec_time(\"Load models\"):\n",
    "        pipelines = dict()\n",
    "        for m in models:\n",
    "            try: \n",
    "                pipelines[m] = init_model(m, torch_dtype=torch.float16)\n",
    "            except OSError:\n",
    "                pipelines[m] = init_model(m, torch_dtype=torch.float16, local_files_only=False)\n",
    "            \n",
    "        \n",
    "    if torch.cuda.is_available():\n",
    "        pseudo_rand_gen = torch.Generator(device='cuda')\n",
    "    else:\n",
    "        pseudo_rand_gen = torch.Generator()\n",
    "\n",
    "    pn.state.cache['pipelines'] = pipelines\n",
    "    pn.state.cache['pseudo_rand_gen'] = pseudo_rand_gen\n",
    "    print(f\"Save to cache\")\n",
    "\n",
    "default_model = next(iter(pipelines))\n",
    "    \n",
    "def generate_image(\n",
    "    prompt,\n",
    "    negative_prompt=None,\n",
    "    model=default_model,\n",
    "    height=512,\n",
    "    width=512,\n",
    "    guidance_scale=7.5,\n",
    "    num_inference_steps=30,\n",
    "    seed=None,\n",
    "):\n",
    "    pipe = pipelines[model]\n",
    "    \n",
    "    if not seed or seed < random_int_range[0]:\n",
    "        seed = random.randint(*random_int_range)\n",
    "    \n",
    "    generator = pseudo_rand_gen.manual_seed(seed)\n",
    "    res = pipe(prompt=prompt,\n",
    "               negative_prompt=negative_prompt,\n",
    "               guidance_scale=guidance_scale,\n",
    "               height=height,\n",
    "               width=width,\n",
    "               num_inference_steps=num_inference_steps,\n",
    "               generator=generator,\n",
    "              )\n",
    "    return res.images[0], seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940ce029-7e18-47cb-a3c9-b4dbb3a40690",
   "metadata": {},
   "source": [
    "The various panel widgets in this code block affect the image generation. When rendered with a template, the sidebar should ideally start out collapsed with only the `Prompt` text box visible. A user writes a prompt, hits enter which triggers the callback to invoke the image generation function. Opening the sidebar provides more options. A user can set various options, then click `Generate` to create image with those options or hit enter on the prompt. If the prompt does not change, hitting enter will not generate a new image - use the `Generate` button to create new images with the same prompt. Below is a description of each option.\n",
    "\n",
    "\n",
    "__Prompt__: Enter a text you wish to use for image generation. Some examples below:\n",
    "\n",
    "  1. Wildflowers on a mountain side \n",
    "  1. A dream of a distant planet, with multiple moons\n",
    "  1. valley of flowers in the Himalayas\n",
    "  \n",
    "__Negative Prompt__: Negative prompt is what the model will try to remove from the image. For instance, in example (1) above, you can add `yellow` to negative prompt to remove yellow flowers\n",
    "\n",
    "__Pretrained Model__: These are the models, download from hugging face, used for inference.\n",
    "\n",
    "__Height, Width__: Height and width in pixels of the images.\n",
    "\n",
    "__Guidance Scale__: Also known as CFG (Classifier-free guidance scale). Typically use a value between 7 to 8.5. As you increase this value, the model will try to match the prompt at the expense of image quality or diversity of the image.\n",
    "\n",
    "__# of steps__: The number of denoising steps taken by the model. As you increase the number of steps the image gets more refined; however, it takes longer to generate.\n",
    "\n",
    "__Seed__: The random seed used when create the noise for the image. This is randomly generated and used for each image. It can be manually set by selecting this checkbox. To reproduce an image, select this option, then copy/paste the URL. Be sure to uncheck the box to revert back to randomly generating the images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3968a727",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Create display widgets and bind them to generate_image function\n",
    "#\n",
    "# save the history in a rolling queue\n",
    "history = deque([], 5)\n",
    "\n",
    "# TODO: docorate function with @pn.cache https://panel.holoviz.org/user_guide/Performance_and_Debugging.html#caching-in-memory\n",
    "def generate_image_with_options(event, prompt):\n",
    "    if not prompt:\n",
    "        return pn.Spacer(height=height.value)\n",
    "    \n",
    "    # also sync the URL bar with options so we can copy/paste to get the exact same image\n",
    "    #\n",
    "    # TODO: \n",
    "    #    Needed these in the callback to have them included in the URL (reproduce in simple example and log an issue)\n",
    "    #    can we improve API so sync can take a list?\n",
    "    #\n",
    "    # unsync if user un-checks the seed button - use this as a proxy to make URL available to share\n",
    "    if pn.state.location:\n",
    "        for w in widgets_for_url_sync:\n",
    "            pn.state.location.sync(*w)\n",
    "\n",
    "    if not set_seed.value:\n",
    "        seed.value = random.randint(*random_int_range)\n",
    "    \n",
    "    print(\"----------------------------\")\n",
    "    print(f\"Seed provided: {seed.value}\")\n",
    "    with exec_time(\"Generate image\"):\n",
    "        image, image_seed = generate_image(prompt=prompt, negative_prompt=neg_prompt.value,\n",
    "                                           model=model.value, height=height.value, width=width.value,\n",
    "                                           guidance_scale=guidance_scale.value, num_inference_steps=num_inference_steps.value,\n",
    "                                           seed=seed.value)\n",
    "\n",
    "    print(f\"Seed used: {image_seed}\")\n",
    "    print(\"------------------------\")\n",
    "\n",
    "    # resize the image and add it to the history\n",
    "    # TODO: also need to add parameters used to create image. It should be a clickable so image can be reconstructed\n",
    "    history.append(image.resize((100, 100)))\n",
    "    if len(prompt_history) == len(history):\n",
    "        prompt_history[0:] = [*history]\n",
    "    else:\n",
    "        # keep appendijng history\n",
    "        prompt_history.append(history[-1])\n",
    "        \n",
    "    return image\n",
    "\n",
    "\n",
    "## Widgets on main page\n",
    "prompt = pn.widgets.TextInput(name='Prompt', value=None)\n",
    "\n",
    "## Widgets in the sidebar\n",
    "neg_prompt = pn.widgets.TextInput(name='Negative Prompt', value=None)\n",
    "\n",
    "model = pn.widgets.Select(name='Pretrained Model', options=list(pipelines), value=default_model)\n",
    "\n",
    "size_range = [448 + i*2**6 for i in range(10)]\n",
    "height = pn.widgets.DiscreteSlider(name='Height', options=size_range, value=size_range[1])\n",
    "width = pn.widgets.DiscreteSlider(name='Width', options=size_range, value=size_range[1])\n",
    "\n",
    "# The CFG scale adjusts how much the image looks closer to the prompt and/ or input image. \n",
    "# If CFG Scale is greater, the output will be more in line with the input prompt and/or input image, but it will be distorted. \n",
    "# The lower the CFG Scale value, the more likely it is to drift away from the prompt or the input image, but the better quality.\n",
    "#\n",
    "guidance_scale = pn.widgets.FloatSlider(start=5, end=10, step=0.1, value=7.5, \n",
    "                                        format=PrintfTickFormatter(format='%.1f'),\n",
    "                                        name='Guidance scale')\n",
    "num_inference_steps = pn.widgets.IntSlider(name='# of steps', start=10, end=75, value=30)\n",
    "\n",
    "\n",
    "# add this to URL when it is copied\n",
    "set_seed = pn.widgets.Checkbox(name='Fix seed', width=140, height=50, visible=True, value=False)\n",
    "\n",
    "# Don't even display it to the user - just use it to figure out whether to use the seed from URL or generate a new one\n",
    "seed = pn.widgets.IntInput(name='', value=random.randint(*random_int_range), \n",
    "                           start=random_int_range[0], end=random_int_range[1], step=10, visible=False, width=140)\n",
    "\n",
    "#updating_share_url = False\n",
    "def make_seed_visible(enable):\n",
    "    # make the seed value visible\n",
    "    seed.visible = enable\n",
    "\n",
    "pn.bind(make_seed_visible, set_seed, watch=True)\n",
    "\n",
    "gen_button = pn.widgets.Button(name='Generate', button_type='primary')\n",
    "\n",
    "model_output = pn.param.ParamFunction(pn.bind(generate_image_with_options, gen_button, prompt))\n",
    "\n",
    "# TODO: Neet something clickable here so we can generate image from the history / pick it up from cache\n",
    "prompt_history = pn.FlexBox()\n",
    "\n",
    "sidebar_widgets = [\n",
    "    neg_prompt,\n",
    "    model,\n",
    "    height,\n",
    "    width,\n",
    "    guidance_scale,\n",
    "    num_inference_steps,\n",
    "    pn.Row(set_seed, seed),\n",
    "    gen_button,\n",
    "]\n",
    "if pn.state.location:\n",
    "    pn.state.location.sync(prompt, {'value': 'prompt'})\n",
    "    pn.state.location.sync(set_seed, {'value': 'set_seed'})\n",
    "\n",
    "# widgets for URL sync\n",
    "# TODO: see if we can use this list to perhaps cache images\n",
    "widgets_for_url_sync =[\n",
    "    #(prompt, {'value': 'prompt'}),\n",
    "    (neg_prompt, {'value': 'neg_prompt'}),\n",
    "    (model, {'value': 'model'}),\n",
    "    (height, {'value': 'height'}),\n",
    "    (width, {'value': 'width'}),\n",
    "    (guidance_scale, {'value': 'cfg'}),\n",
    "    (num_inference_steps, {'value': 'steps'}),\n",
    "    (seed, {'value': 'seed'}),\n",
    "    #(set_seed, {'value': 'set_seed'})\n",
    "]\n",
    "\n",
    "## logo / headers / \n",
    "logo  = \"\"\"<a href=\"http://panel.pyviz.org\">\n",
    "           <img src=\"https://panel.pyviz.org/_static/logo_stacked.png\" \n",
    "            width=150 height=127 align=\"left\" margin=20px>\"\"\"\n",
    "\n",
    "title = 'Stable Diffusion with Panel UI'\n",
    "\n",
    "desc = pn.pane.HTML(\"\"\"\n",
    "    The <a href=\"http://panel.pyviz.org\">Panel</a> library from <a href=\"https://holoviz.org/\">HoloViz</a> \n",
    "    lets you make widget-controlled apps. Here you can use the\n",
    "    <a href=\"https://huggingface.co/docs/diffusers/index\">diffusers</a> library to\n",
    "    generate images from pretrained diffusion models. Panel is used to create the UI for the pipeline.\"\"\", width=250)\n",
    "\n",
    "## Customize image generation layout\n",
    "tweak_image_gen = pn.Column(*sidebar_widgets)\n",
    "\n",
    "## Final display\n",
    "output = pn.Column(model_output, prompt_history, sizing_mode='stretch_width')\n",
    "pn.Column(logo, title, desc,\n",
    "    prompt, \n",
    "    pn.Row(tweak_image_gen, output),\n",
    "    sizing_mode='stretch_width')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a5b292",
   "metadata": {},
   "source": [
    "### Use a template\n",
    "\n",
    "Use a template to get a clean look and feel.\n",
    "\n",
    "TODO: Start out with the sidebar collapsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6656725f",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = pn.template.MaterialTemplate(\n",
    "    title=title,\n",
    ")\n",
    "\n",
    "template.sidebar.append(logo)\n",
    "template.sidebar.append(desc.clone(width=300, margin=(20, 5)))\n",
    "template.sidebar.append(tweak_image_gen)\n",
    "\n",
    "template.main.append(pn.Column(prompt, output, sizing_mode='stretch_width'))\n",
    "\n",
    "template.servable();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

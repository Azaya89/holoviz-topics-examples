{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c10ed41-57e2-4e7f-a34d-16c94dccbb67",
   "metadata": {},
   "source": [
    "# Stable Diffusion with Panel UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35069f42-8e53-4644-9aec-6e15f883e846",
   "metadata": {
    "tags": []
   },
   "source": [
    "## IMPORTANT: Note for use on MacOS M1\n",
    "\n",
    "At present this is tested for deployment on osx-M1, linux-64 with Nvidia GPUs (Quadro RTX 8000) and linux-64 with only CPUs. The `default` environment for this `anaconda-project.yml` targets a `linux-64` with GPU or only CPU. On osx-M1, the CPU only version raises a torch error. On osx with M1 chip run using the `--env-spec` variable as follows:\n",
    "\n",
    "```\n",
    "anaconda-project run --env-spec stable-diffusion-m1 dashboard\n",
    "```\n",
    "\n",
    "If you want to execute the following code cells on osx, you'll also need to run it with the `stable-diffiusion-m1` env-spec:\n",
    "\n",
    "```\n",
    "anaconda-project run --env-spec stable-diffusion-m1\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## Stable Diffusion, Diffusers library\n",
    "\n",
    "[Stable Diffusion](https://en.wikipedia.org/wiki/Stable_Diffusion#:~:text=Stable%20Diffusion%20is%20a%20deep,guided%20by%20a%20text%20prompt) is a deep learning, text-to-image model released in 2022. It is primarily used to generate detailed images conditioned on text descriptions. \n",
    "\n",
    "This example uses the [Diffusers library](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb) with checkpoints from the runwayml and CompVis repositories. [Diffusers on github](https://github.com/huggingface/diffusers#stable-diffusion-is-fully-compatible-with-diffusers). Blogpost on [Stable Diffusion with Diffusers](https://huggingface.co/blog/stable_diffusion)\n",
    "\n",
    "### Performance: GPU\n",
    "\n",
    "The example assumes it will run on a GPU. It can be modified to run on a CPU but image generation will take on the order of minutes as opposed to seconds.\n",
    "\n",
    "\n",
    "### Limitations\n",
    "\n",
    "The models were trained on images with resolution of 512x512. The diffusers pipeline and subsequently the UI allows creation of images with different resolutions; however, the image quality degrades if deviating from the resolution used to train the model. \n",
    "\n",
    "\n",
    "### Seed\n",
    "\n",
    "The idea behind stable diffusion is to start with a noisy image, with the goal of removing gaussian noise in each inference step. The seed value determines the randomness and the output generated. By default the seed is randomized in this application with the opportunity to explore generated images for the same prompt. The parameters used to generate an image is captured in the URL. As noted above, changing the resolution will also change the image output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1227ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import torch\n",
    "import random\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "from bokeh.models.formatters import PrintfTickFormatter\n",
    "import panel as pn\n",
    "import param\n",
    "from panel.layout.base import ListLike\n",
    "from panel.reactive import ReactiveHTML\n",
    "from panel.viewable import Viewer, Viewable\n",
    "\n",
    "pn.extension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211fb06b-cdf8-491d-abf8-55569da913fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a context manager to measure execution time and print it to the console\n",
    "@contextmanager\n",
    "def exec_time(description=\"Task\"):\n",
    "    st = time.perf_counter()\n",
    "    yield \n",
    "    print(f\"{description}: {time.perf_counter() - st:.2f} sec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3659bb1-aa2c-4f30-b2c9-f4957bdb2b81",
   "metadata": {},
   "source": [
    "The `init_model` function will first look in the default cache location used by huggingface to find downloaded pretrained model. If these haven't been downloaded yet, it will first download the models. On subsequent restarts of the app, it'll load the models from the local cache. These can also be downloaded separately as follows:\n",
    "  \n",
    "  ```\n",
    "  pipe, cache = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", return_cached_folder=True, local_files_only=False)\n",
    "  pipe, cache = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", return_cached_folder=True, local_files_only=False)\n",
    "  print(cache) # to see the default cache location\n",
    "  ```\n",
    "\n",
    "In addition to caching the pretrained model, we also initialize and cache the diffusers pipeline inside `panel.state.cache`. This ensures that each new visitor to the page does not require creating a new diffusers pipeline.\n",
    "The initial page load takes an extra ~10 sec or so (on a Qadro RTX 8000) and allocates the GPU memory required to load the pipeline in memory but subsequent visitors get this pipeline from panel's cache. The memory overhead from here is the amount needed to generate the image  text prompt.\n",
    "Below is an example output of the `nvidia-smi` running on a machine with Quadro RTX 8000 GPUs, after both models load.\n",
    "\n",
    "```\n",
    "+-----------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
    "|-------------------------------+----------------------+----------------------+\n",
    "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                               |                      |               MIG M. |\n",
    "|===============================+======================+======================|\n",
    "|   0  Quadro RTX 8000     Off  | 00000000:15:00.0 Off |                  Off |\n",
    "| 33%   33C    P8    24W / 260W |     48MiB / 49152MiB |      0%      Default |\n",
    "|                               |                      |                  N/A |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "|   1  Quadro RTX 8000     Off  | 00000000:2D:00.0 Off |                  Off |\n",
    "| 33%   40C    P8    29W / 260W |   5933MiB / 49152MiB |      0%      Default |\n",
    "|                               |                      |                  N/A |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "\n",
    "+-----------------------------------------------------------------------------+\n",
    "| Processes:                                                                  |\n",
    "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
    "|        ID   ID                                                   Usage      |\n",
    "|=============================================================================|\n",
    "|    0   N/A  N/A      2024      G   /usr/lib/xorg/Xorg                 23MiB |\n",
    "|    0   N/A  N/A      2545      G   /usr/bin/gnome-shell               20MiB |\n",
    "|    1   N/A  N/A      2024      G   /usr/lib/xorg/Xorg                  4MiB |\n",
    "|    1   N/A  N/A   2263594      C   .../diffusers/bin/python3.11     5925MiB |\n",
    "+-----------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75f0c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize models and define function for image generation\n",
    "# use only downloaded models\n",
    "random_int_range = 1, int(1e6)\n",
    "\n",
    "def init_model(model, cuda, mps, local_files_only=True):\n",
    "    print(f\"Init model: {model}\")\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(\n",
    "        model,\n",
    "        torch_dtype=torch.float16 if cuda or mps else None,\n",
    "        local_files_only=local_files_only)        \n",
    "\n",
    "    # let torch choose the GPU if more than 1 is available\n",
    "    if cuda:\n",
    "        pipe.to(f\"cuda\")\n",
    "    elif mps:\n",
    "        pipe.to(f\"mps\")\n",
    "        pipe.enable_attention_slicing()\n",
    "    return pipe     \n",
    "\n",
    "\n",
    "if 'pipelines' in pn.state.cache:\n",
    "    print(f\"load from cache\")\n",
    "    pipelines = pn.state.cache['pipelines']\n",
    "    pseudo_rand_gen = pn.state.cache['pseudo_rand_gen']\n",
    "else:\n",
    "    cuda = True if torch.cuda.is_available() else False\n",
    "    mps = True if torch.backends.mps.is_available() else False\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    models = ['runwayml/stable-diffusion-v1-5', \n",
    "              'CompVis/stable-diffusion-v1-4'\n",
    "             ]\n",
    "    \n",
    "    pseudo_rand_gen = torch.Generator(device=device)\n",
    "    with exec_time(\"Load models\"):\n",
    "        pipelines = dict()\n",
    "        for m in models:\n",
    "            try:\n",
    "                # try to load files from cache first\n",
    "                pipelines[m] = init_model(m, cuda, mps)\n",
    "            except OSError:\n",
    "                pipelines[m] = init_model(m, cuda, mps, local_files_only=False)\n",
    "            \n",
    "    pn.state.cache['pipelines'] = pipelines\n",
    "    pn.state.cache['pseudo_rand_gen'] = pseudo_rand_gen\n",
    "    print(f\"Save to cache\")\n",
    "\n",
    "\n",
    "default_model = next(iter(pipelines))\n",
    "\n",
    "\n",
    "def generate_image(\n",
    "    prompt,\n",
    "    neg_prompt=None,\n",
    "    model=default_model,\n",
    "    height=512,\n",
    "    width=512,\n",
    "    guidance_scale=7.5,\n",
    "    num_steps=30,\n",
    "    seed=None,\n",
    "):\n",
    "    pipe = pipelines[model]\n",
    "    \n",
    "    if not seed or seed < random_int_range[0]:\n",
    "        seed = random.randint(*random_int_range)\n",
    "    \n",
    "    generator = pseudo_rand_gen.manual_seed(seed)\n",
    "    res = pipe(prompt=prompt,\n",
    "               negative_prompt=neg_prompt,\n",
    "               guidance_scale=guidance_scale,\n",
    "               height=height,\n",
    "               width=width,\n",
    "               num_inference_steps=num_steps,\n",
    "               generator=generator,\n",
    "              )\n",
    "    return res.images[0], seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940ce029-7e18-47cb-a3c9-b4dbb3a40690",
   "metadata": {},
   "source": [
    "The various panel widgets in this code block affect the image generation. When rendered with a template, the sidebar should ideally start out collapsed with only the `Prompt` text box visible. Opening the sidebar provides more options. A user can set various options, then click `Generate` to create image with those options. \n",
    "\n",
    "__Prompt__: Enter a text you wish to use for image generation. Some examples below:\n",
    "\n",
    "  1. Wildflowers on a mountain side \n",
    "  1. A dream of a distant planet, with multiple moons\n",
    "  1. valley of flowers in the Himalayas\n",
    "  \n",
    "__Negative Prompt__: Negative prompt is what the model will try to remove from the image. For instance, in example (1) above, you can add `yellow` to negative prompt to remove yellow flowers\n",
    "\n",
    "__Pretrained Model__: These are the models, download from hugging face, used for inference.\n",
    "\n",
    "__Height, Width__: Height and width in pixels of the images.\n",
    "\n",
    "__Guidance Scale__: Also known as CFG (Classifier-free guidance scale). Typically use a value between 7 to 8.5. As you increase this value, the model will try to match the prompt at the expense of image quality or diversity of the image.\n",
    "\n",
    "__# of steps__: The number of denoising steps taken by the model. As you increase the number of steps the image gets more refined; however, it takes longer to generate.\n",
    "\n",
    "The random seed used when create the noise for the image is randomly set for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3968a727",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gallery(ListLike, ReactiveHTML):\n",
    "    \n",
    "    objects = param.List(item_type=Viewable)\n",
    "   \n",
    "    current = param.Integer(default=None)\n",
    "    \n",
    "    margin = param.Integer(0)\n",
    "\n",
    "    _template = \"\"\"\n",
    "    <div id=\"gallery\" style=\"display: grid; width: 350; height: 550; grid-template-columns: 1fr 1fr 1fr;\">\n",
    "    {% for img in objects %}\n",
    "      <div id=\"img\" name=\"{{ img.name }}\" onclick=${script('click')}>${img}</div>\n",
    "    {% endfor %}\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    _scripts = {\n",
    "        'click': \"\"\"\n",
    "          const id = event.target.parentNode.parentNode.parentNode.id;\n",
    "          data.current = Number(id.split('-')[1]);\n",
    "          \"\"\"\n",
    "    }\n",
    "\n",
    "class StableDiffusionUI(Viewer):\n",
    "    \n",
    "    prompt = param.String(label='Prompt')\n",
    "    \n",
    "    neg_prompt = param.String(label='Negative Prompt')\n",
    "    \n",
    "    model = param.Selector(objects=list(pipelines), default=default_model)\n",
    "    \n",
    "    _size_range = tuple(448 + i*2**6 for i in range(10))\n",
    "    width = param.Selector(_size_range, default=_size_range[1])\n",
    "    \n",
    "    height = param.Selector(_size_range, default=_size_range[1])\n",
    "    \n",
    "    guidance_scale = param.Number(bounds=(5, 10), step=0.1, default=7.5)\n",
    "    \n",
    "    num_steps = param.Integer(label='# of steps', bounds=(10, 75), default=30)\n",
    "    \n",
    "    gallery = param.ClassSelector(class_=Gallery, default=Gallery(min_height=100), precedence=-1)\n",
    "    \n",
    "    seed = param.Integer(\n",
    "        default=None, bounds=random_int_range, step=10,\n",
    "        precedence=-1)\n",
    "    \n",
    "    generate = param.Event(precedence=1)\n",
    "    \n",
    "    def __init__(self, **params):\n",
    "        self.history = []\n",
    "        super().__init__(name=params.pop('name', 'Stable Diffusion with Panel UI'), **params)\n",
    "        self.gallery.param.watch(self._restore_history, 'current')\n",
    "        self._restore = False\n",
    "        self._on_load()\n",
    "\n",
    "    @contextmanager\n",
    "    def _toggle(self, attr: str, value: bool):\n",
    "        # toggle state of bool attribute inside context\n",
    "        # if exception raised by code inside the contextmanager, set the state back to original and rethrow exception\n",
    "        init_state = getattr(self, attr)\n",
    "        try:\n",
    "            setattr(self, attr, value)\n",
    "            yield\n",
    "            setattr(self, attr, not(value))\n",
    "        except Exception as ex:\n",
    "            setattr(self, attr, init_state)\n",
    "            raise ex\n",
    "        \n",
    "\n",
    "    def _restore_history(self, event):\n",
    "        if event.new is None:\n",
    "            return\n",
    "        self.gallery.current = None\n",
    "        self.param.update(self.history[event.new])\n",
    "        with self._toggle('_restore', value=True):\n",
    "            self.param.trigger('generate')\n",
    " \n",
    "    @property\n",
    "    def _current_state(self):\n",
    "        return {'prompt': self.prompt,\n",
    "                'neg_prompt': self.neg_prompt,\n",
    "                'model': self.model,\n",
    "                'height': self.height,\n",
    "                'width': self.width,\n",
    "                'guidance_scale': self.guidance_scale,\n",
    "                'num_steps': self.num_steps,\n",
    "                'seed': self.seed}\n",
    "    \n",
    "    def _on_load(self):\n",
    "        if pn.state.location and pn.state.location.query_params:\n",
    "            self.param.update(pn.state.location.query_params)\n",
    "            self.param.trigger('generate')\n",
    "\n",
    "    # Binding to both widgets creates some unexpected behavior when you go back to a previous image from the\n",
    "    # gallery. The _restore_history() function seems to trigger generate and prompt\n",
    "    #@param.depends('generate', 'prompt')\n",
    "    @param.depends('generate')\n",
    "    def image(self):\n",
    "        if not self.prompt:\n",
    "            return pn.pane.PNG(style={'border': '1px solid black'}, height=self.height, width=self.width)\n",
    "        \n",
    "        # if self._restore is True, the seed will be set \n",
    "        if not self.seed:\n",
    "            self.seed = random.randint(*self.param.seed.bounds)\n",
    "\n",
    "        current_state = self._current_state\n",
    "        with exec_time(f\"Generate {self.prompt}\"):\n",
    "            image, image_seed = generate_image(**current_state)\n",
    "\n",
    "        if not self._restore:\n",
    "            self.gallery.append(pn.pane.PNG(image.resize((100, 100))))\n",
    "            self.history.append({\n",
    "                p: v for p, v in self.param.values().items() if p not in ('name', 'gallery', 'generate')\n",
    "            })\n",
    "\n",
    "        pn.state.location.update_query(**self._current_state)\n",
    "        # reset seed back to None after image generation\n",
    "        self.seed = None\n",
    "        return pn.pane.PNG(image, style={'border': '1px solid black'})\n",
    "\n",
    "    def _sidebar_widgets(self):\n",
    "        return [\n",
    "\n",
    "            self.param.model,\n",
    "            pn.Param(self.param.height, widgets={'height': pn.widgets.DiscreteSlider}),\n",
    "            pn.Param(self.param.width, widgets={'width': pn.widgets.DiscreteSlider}),\n",
    "            pn.Param(self.param.guidance_scale, \n",
    "                     widgets={'guidance_scale': \n",
    "                              {'formatter': PrintfTickFormatter(format='%.1f')}}),\n",
    "            self.param.num_steps,\n",
    "        ]\n",
    "\n",
    "    def _main_widgets(self):\n",
    "        return [\n",
    "            pn.Row(\n",
    "                pn.Column(self.param.prompt, self.param.neg_prompt, sizing_mode='stretch_width'),\n",
    "                pn.Param(self.param.generate, \n",
    "                         widgets={'generate': {'button_type': 'success', 'height': 110, 'width': 30, 'name': 'Generate Image'}}),\n",
    "            ),\n",
    "            pn.Row(pn.param.ParamMethod(self.image, loading_indicator=True),\n",
    "                   pn.Column(self.gallery))\n",
    "        ]\n",
    "    \n",
    "    def __panel__(self):\n",
    "        # Discrete slider for width, height: https://huggingface.co/blog/stable_diffusion\n",
    "        return pn.Row(\n",
    "            pn.Column(*self._sidebar_widgets()),\n",
    "            pn.Column(*self._main_widgets(), sizing_mode='stretch_width')\n",
    "        )\n",
    "\n",
    "    \n",
    "sdui = StableDiffusionUI()\n",
    "\n",
    "sdui"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a5b292",
   "metadata": {},
   "source": [
    "### Use a template\n",
    "\n",
    "Use a template to get a clean look and feel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6656725f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## logo / headers / \n",
    "logo_pn  = \"\"\"<a href=\"http://panel.pyviz.org\">\n",
    "           <img src=\"https://panel.pyviz.org/_static/logo_stacked.png\" \n",
    "            width=108 height=91 align=\"left\" margin=10px>\"\"\"\n",
    "\n",
    "logo_diffusers  = \"\"\"<a href=\"https://huggingface.co/docs/diffusers/index\">\n",
    "           <img src=\"./thumbnails/diffusers_logo.png\" \n",
    "            width=184 height=100 align=\"left\" margin=10px>\"\"\"\n",
    "\n",
    "desc = pn.pane.HTML(\"\"\"\n",
    "    The <a href=\"http://panel.pyviz.org\">Panel</a> library from <a href=\"https://holoviz.org/\">HoloViz</a> \n",
    "    lets you make widget-controlled apps. Here you can use the\n",
    "    <a href=\"https://huggingface.co/docs/diffusers/index\">diffusers</a> library to\n",
    "    generate images from pretrained diffusion models. Panel is used to create the UI for the pipeline.\"\"\", width=250)\n",
    "\n",
    "template = pn.template.MaterialTemplate(\n",
    "    title=sdui.name,\n",
    ")\n",
    "\n",
    "template.sidebar.append(pn.Row(logo_pn, logo_diffusers))\n",
    "template.sidebar.append(desc.clone(width=300, margin=(20, 5)))\n",
    "template.sidebar.append(pn.Column(*sdui._sidebar_widgets()))\n",
    "\n",
    "template.main.append(pn.Column(*sdui._main_widgets(), sizing_mode='stretch_width'))\n",
    "\n",
    "template.servable();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
